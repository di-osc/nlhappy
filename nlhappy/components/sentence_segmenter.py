from spacy.pipeline import Sentencizer
from spacy.lang.zh import Chinese
from spacy.language import Language
from typing import List, Optional, Tuple, Callable
from spacy.pipeline import Pipe
import spacy
import re

default_punct_chars = ['!',  '?', 'Ö‰', 'ØŸ', 'Û”', 'Ü€', 'Ü', 'Ü‚', 'ß¹',
            'à¥¤', 'à¥¥', 'áŠ', 'á‹', 'á¢', 'á§', 'á¨', 'á™®', 'áœµ', 'áœ¶', 'á ƒ', 'á ‰', 'á¥„',
            'á¥…', 'áª¨', 'áª©', 'áªª', 'áª«', 'á­š', 'á­›', 'á­', 'á­Ÿ', 'á°»', 'á°¼', 'á±¾', 'á±¿',
            'â€¼', 'â€½', 'â‡', 'âˆ', 'â‰', 'â¸®', 'â¸¼', 'ê“¿', 'ê˜', 'ê˜', 'ê›³', 'ê›·', 'ê¡¶',
            'ê¡·', 'ê£', 'ê£', 'ê¤¯', 'ê§ˆ', 'ê§‰', 'ê©', 'ê©', 'ê©Ÿ', 'ê«°', 'ê«±', 'ê¯«', 'ï¹’',
            'ï¹–', 'ï¹—', 'ï¼', 'ï¼', 'ï¼Ÿ', 'ğ©–', 'ğ©—', 'ğ‘‡', 'ğ‘ˆ', 'ğ‘‚¾', 'ğ‘‚¿', 'ğ‘ƒ€',
            'ğ‘ƒ', 'ğ‘…', 'ğ‘…‚', 'ğ‘…ƒ', 'ğ‘‡…', 'ğ‘‡†', 'ğ‘‡', 'ğ‘‡', 'ğ‘‡Ÿ', 'ğ‘ˆ¸', 'ğ‘ˆ¹', 'ğ‘ˆ»', 'ğ‘ˆ¼',
            'ğ‘Š©', 'ğ‘‘‹', 'ğ‘‘Œ', 'ğ‘—‚', 'ğ‘—ƒ', 'ğ‘—‰', 'ğ‘—Š', 'ğ‘—‹', 'ğ‘—Œ', 'ğ‘—', 'ğ‘—', 'ğ‘—', 'ğ‘—',
            'ğ‘—‘', 'ğ‘—’', 'ğ‘—“', 'ğ‘—”', 'ğ‘—•', 'ğ‘—–', 'ğ‘——', 'ğ‘™', 'ğ‘™‚', 'ğ‘œ¼', 'ğ‘œ½', 'ğ‘œ¾', 'ğ‘©‚',
            'ğ‘©ƒ', 'ğ‘ª›', 'ğ‘ªœ', 'ğ‘±', 'ğ‘±‚', 'ğ–©®', 'ğ–©¯', 'ğ–«µ', 'ğ–¬·', 'ğ–¬¸', 'ğ–­„', 'ğ›²Ÿ', 'ğªˆ',
            'ï½¡', 'ã€‚', 'ï¼Ÿ', 'ï¼', '......', 'â€¦â€¦', ';', 'ï¼›', '.']

@Chinese.factory(name='sentence_segmenter', default_config={'punct_chars': default_punct_chars})
def make_sentence_segmenter(nlp, name, punct_chars):
    return Sentencizer(name, punct_chars=punct_chars)




def cut_sentences_v1(sent):
    """
    the first rank of sentence cut
    """
    sent = re.sub('([ã€‚ï¼Ÿï¼\?])([^â€â€™])', r"\1\n\2", sent)  # å•å­—ç¬¦æ–­å¥ç¬¦
    sent = re.sub('(\.{6})([^â€â€™])', r"\1\n\2", sent)  # è‹±æ–‡çœç•¥å·
    sent = re.sub('(\â€¦{2})([^â€â€™])', r"\1\n\2", sent)  # ä¸­æ–‡çœç•¥å·
    sent = re.sub('([ã€‚ï¼ï¼Ÿ\?][â€â€™])([^ï¼Œã€‚ï¼ï¼Ÿ\?])', r"\1\n\2", sent)
    # å¦‚æœåŒå¼•å·å‰æœ‰ç»ˆæ­¢ç¬¦ï¼Œé‚£ä¹ˆåŒå¼•å·æ‰æ˜¯å¥å­çš„ç»ˆç‚¹ï¼ŒæŠŠåˆ†å¥ç¬¦\næ”¾åˆ°åŒå¼•å·å
    return sent.split("\n")

def cut_sentences_v2(sent):
    """
    the second rank of spilt sentence, split 'ï¼›' | ';'
    """
    sent = re.sub('([ï¼›;])([^â€â€™])', r"\1\n\2", sent)
    return sent.split("\n")

def cut_sent(text, max_seq_len):
    # å°†å¥å­åˆ†å¥ï¼Œç»†ç²’åº¦åˆ†å¥åå†é‡æ–°åˆå¹¶
    sentences = []

    # ç»†ç²’åº¦åˆ’åˆ†
    sentences_v1 = cut_sentences_v1(text)
    for sent_v1 in sentences_v1:
        if len(sent_v1) > max_seq_len - 2:
            sentences_v2 = cut_sentences_v2(sent_v1)
            sentences.extend(sentences_v2)
        else:
            sentences.append(sent_v1)
    assert ''.join(sentences) == text

    # åˆå¹¶
    merged_sentences = []
    start_index_ = 0

    while start_index_ < len(sentences):
        tmp_text = sentences[start_index_]

        end_index_ = start_index_ + 1

        while end_index_ < len(sentences) and \
                len(tmp_text) + len(sentences[end_index_]) <= max_seq_len - 2:
            tmp_text += sentences[end_index_]
            end_index_ += 1

        start_index_ = end_index_

        merged_sentences.append(tmp_text)

    return merged_sentences


if __name__ == '__main__':
    nlp = spacy.blank('zh')
    nlp.add_pipe('sentence_segmenter')
    text = "è‚¥åšå‹å¿ƒè‚Œç—…@æœ‰é˜µå‘æ€§æˆ–æ…¢æ€§å¿ƒæˆ¿é¢¤åŠ¨çš„ HCM æ‚£è€…ä½¿ç”¨åæ³•æ—æŠ—å‡çš„å›½é™…æ ‡å‡†åŒ–æ¯”å€¼ (INR) ç›®æ ‡å€¼æ¨èä¸º 2.0-3.0ã€‚"
    doc = nlp(text)
    print(list(doc.sents))

