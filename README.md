
<div align='center'>

# NLHappy
<a href="https://pytorch.org/get-started/locally/"><img alt="PyTorch" src="https://img.shields.io/badge/PyTorch-ee4c2c?logo=pytorch&logoColor=white"></a>
<a href="https://pytorchlightning.ai/"><img alt="Lightning" src="https://img.shields.io/badge/-Lightning-792ee5?logo=pytorchlightning&logoColor=white"></a>
<a href="https://hydra.cc/"><img alt="Config: Hydra" src="https://img.shields.io/badge/Config-Hydra-89b8cd"></a>
<a href="https://github.com/ashleve/lightning-hydra-template"><img alt="Template" src="https://img.shields.io/badge/-Lightning--Hydra--Template-017F2F?style=flat&logo=github&labelColor=gray"></a>
<a href="https://spacy.io/"><img alt="Spacy" src="https://img.shields.io/badge/component-%20Spacy-blue"></a>
<a href="https://wandb.ai/"><img alt="WanDB" src="https://img.shields.io/badge/Log-WanDB-brightgreen"></a>
</div>
<br><br>

## ğŸ“Œ&nbsp;&nbsp; ç®€ä»‹

nlhappyè‡´åŠ›äºå¿«é€Ÿå®ŒæˆNLPä»»åŠ¡,ä½ å”¯ä¸€éœ€è¦åšçš„å°±æ˜¯å°†æ•°æ®å¤„ç†ä¸ºä»»åŠ¡å¯¹åº”çš„æ•°æ®ç±».
> å®ƒä¸»è¦çš„ä¾èµ–æœ‰
- [transformers](https://huggingface.co/docs/transformers/index): ä¸‹è½½é¢„è®­ç»ƒæƒé‡
- [pytorch-lightning](https://pytorch-lightning.readthedocs.io/en/latest/): æ¨¡å‹è®­ç»ƒ
- [datasets](https://huggingface.co/docs/datasets/index): æ„å»ºæ•°æ®é›†
- [pydantic](https://wandb.ai/): æ„å»ºæ•°æ®æ¨¡å‹


## ğŸš€&nbsp;&nbsp; å®‰è£…
<details>
<summary><b>å®‰è£…nlhappy</b></summary>

> æ¨èå…ˆå»[pytorchå®˜ç½‘](https://pytorch.org/get-started/locally/)å®‰è£…pytorchå’Œå¯¹åº”cuda
```bash
# pip å®‰è£…
pip install --upgrade pip
pip install --upgrade nlhappy
```
</details>

<details>
<summary><b>å…¶ä»–å¯é€‰</b></summary>

> æ¨èå®‰è£…wandbç”¨äºå¯è§†åŒ–è®­ç»ƒæ—¥å¿—
- æ³¨å†Œ: https://wandb.ai/
- è·å–è®¤è¯: https://wandb.ai/authorize
- ç™»é™†:
```bash
wandb login
```
æ¨¡å‹è®­ç»ƒå¼€å§‹åå»[å®˜ç½‘](https://wandb.ai/)æŸ¥çœ‹è®­ç»ƒå®å†µ
</details>




## âš¡&nbsp;&nbsp; å¼€å§‹ä»»åŠ¡

<details>
<summary><b>æ–‡æœ¬åˆ†ç±»</b></summary>

> æ•°æ®å¤„ç†
```python
from nlhappy.utils.make_doc import Doc, DocBin
from nlhappy.utils.make_dataset import DatasetDict
# æ„å»ºcorpus
# å°†æ•°æ®å¤„ç†ä¸ºç»Ÿä¸€çš„Docå¯¹è±¡,å®ƒå­˜å‚¨ç€æ‰€æœ‰æ ‡ç­¾æ•°æ®
docs = []
# dataä¸ºä½ è‡ªå·±çš„æ•°æ®
# doc._.label ä¸ºæ–‡æœ¬çš„æ ‡ç­¾,ä¹‹æ‰€ä»¥åŠ '_'æ˜¯å› ä¸ºè¿™æ˜¯spacy Docä¿å­˜ç”¨æˆ·è‡ªå·±æ•°æ®çš„ç”¨æ³•
for d in data:
    doc = nlp(d['text'])
    doc._.label = d['label']
    docs.append(doc)
# ä¿å­˜corpus,æ–¹ä¾¿åè¾¹badcaseåˆ†æ
db = DocBin(docs=docs, store_user_data=True)
# æ–°é—»æ–‡æœ¬-Tag3ä¸ºä¿å­˜æ ¼å¼ç›®å½•,éœ€è¦æ›´æ¢ä¸ºè‡ªå·±çš„å½¢å¼
db.to_disk('corpus/TNEWS-Tag15/train.spacy')
# æ„å»ºæ•°æ®é›†,ä¸ºäº†è®­ç»ƒæ¨¡å‹
ds = convert_docs_to_tc_dataset(docs=docs)
# ä½ å¯ä»¥å°†æ•°æ®é›†è½¬æ¢ä¸ºdataframeè¿›è¡Œå„ç§åˆ†æ,æ¯”å¦‚è·å–æ–‡æœ¬æœ€å¤§é•¿åº¦
df = ds.to_pandas()
max_length = df['text'].str.len().max()
# æ•°æ®é›†åˆ‡åˆ†
dsd = train_val_split(ds, val_frac=0.2)
# ä¿å­˜æ•°æ®é›†,æ³¨æ„è¦ä¿å­˜åˆ°datasets/ç›®å½•ä¸‹
dsd.save_to_disk('datasets/TNEWS')
```
> è®­ç»ƒæ¨¡å‹

- ç¼–å†™è®­ç»ƒè„šæœ¬,scripts/train.sh
```
nlhappy \
datamodule=text_classification \
datamodule.dataset=TNEWS \
datamodule.plm=hfl/chinese-roberta-wwm-ext \
datamodule.batch_size=32 \
model=bert_tc \
model.lr=3e-5 \
seed=1234
# é»˜è®¤ä¸ºå•gpu 0å·æ˜¾å¡è®­ç»ƒ,å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼ä¿®æ”¹æ˜¾å¡
# trainer.devices=[1]
# å•å¡åŠç²¾åº¦è®­ç»ƒ
# trainer.precision=16
# ä½¿ç”¨wandbè®°å½•æ—¥å¿—
# logger=wandb
# å¤šå¡è®­ç»ƒ
# trainer=ddp trainer.devices=4
```

- åå°è®­ç»ƒ
```
nohup bash scripts/train.sh >/dev/null 2>&1 &
```
- å¦‚æœè®¾ç½®logger=wandbåˆ™ç°åœ¨å¯ä»¥å»[wandbå®˜ç½‘](https://wandb.ai/)æŸ¥çœ‹è®­ç»ƒè¯¦æƒ…äº†, å¹¶ä¸”ä¼šè‡ªåŠ¨äº§ç”Ÿlogsç›®å½•é‡Œé¢åŒ…å«äº†è®­ç»ƒçš„ckpt,æ—¥å¿—ç­‰ä¿¡æ¯.

> æ„å»ºè‡ªç„¶è¯­è¨€å¤„ç†æµç¨‹,å¹¶æ·»åŠ ç»„ä»¶
```python
import nlhappy

nlp = nlhappy.nlp()
# é»˜è®¤device cpu, é˜ˆå€¼0.8
config = {'device':'cuda:0', 'threshold':0.9}
tc = nlp.add_pipe('text_classifier', config=config)
# logsæ–‡ä»¶å¤¹é‡Œé¢è®­ç»ƒçš„æ¨¡å‹è·¯å¾„
ckpt = 'logs/experiments/runs/TNEWS/date/checkpoints/epoch_score.ckpt/'
tc.init_model(ckpt)
text = 'æ–‡æœ¬'
doc = nlp(text)
# æŸ¥çœ‹ç»“æœ
print(doc.text, doc._.label, doc.cats)
# ä¿å­˜æ•´ä¸ªæµç¨‹
nlp.to_disk('path/nlp')
# åŠ è½½
nlp = nlhappy.load('path/nlp')
```
> badcaseåˆ†æ
```python
import nlhappy
from nlhappy.utils.make_doc import get_docs_form_docbin
from nlhappy.utils.analysis_doc import analysis_text_badcase, Example

targs = get_docs_from_docbin('corpus/TNEWS-Tag15/train.spacy')
nlp = nlhappy.load('path/nlp')
preds = []
for d in targs:
    doc = nlp(d['text'])
    preds.append(doc)
eg = [Example(x,y) for x,y in zip(preds, targs)]
badcases, score = analysis_text_badcase(eg, return_prf=True)
print(badcases[0].x, badcases[0].x._.label)
print(badcases[0].y, badcases[0].y._.label)
```
> éƒ¨ç½²
- ç›´æ¥ç”¨nlpå¼€å‘æ¥å£éƒ¨ç½²
- è½¬ä¸ºonnx
```python
from nlhappy.models import BertTextClassification
ckpt = 'logs/path/ckpt'
model = BertTextClassification.load_from_ckeckpoint(ckpt)
model.to_onnx('path/tc.onnx')
model.tokenizer.save_pretrained('path/tokenizer')
```
</details>

<details>
<summary><b>å®ä½“æŠ½å–</b></summary>

nlhappyæ”¯æŒåµŒå¥—å’ŒéåµŒå¥—å®ä½“æŠ½å–ä»»åŠ¡
> æ•°æ®å¤„ç†
```python
from nlhappy.utils.convert_doc import convert_spans_to_dataset
from nlhappy.utils.make_doc import get_docs_from_docbin
from nlhappy.utils.make_dataset import train_val_split
import nlhappy
# åˆ¶ä½œdocs
nlp = nlhappy.nlp()
docs = []
# dataä¸ºä½ è‡ªå·±æ ¼å¼çš„åŸå§‹æ•°æ®,æŒ‰éœ€ä¿®æ”¹
# åªéœ€è®¾ç½®doc.ents 
# åµŒå¥—å‹å®ä½“è®¾ç½®doc.spans['all']
for d in data:
    doc = nlp(d['text'])
    # éåµŒå¥—å®ä½“
    ents = []
    for ent in d['spans']:
        start = ent['start']
        end = ent['end']
        label = ent['label']
        span = doc.char_span(start, end, label)
        ents.append(span)
    doc.set_ents(ents)
    docs.append(doc)
    # åµŒå¥—å‹å®ä½“
    for ent in d['spans']:
        start = ent['start']
        end = ent['end']
        label = ent['label']
        span = doc.char_span(start, end, label)
        doc.spans['all'].append(span)
    docs.append(doc)
# ä¿å­˜docs,æ–¹ä¾¿åè¾¹badcaseåˆ†æ
db = DocBin(docs=docs, store_user_data=True)
# åˆ¶ä½œæ•°æ®é›†
# å¦‚æœæ–‡æœ¬è¿‡é•¿å¯ä»¥è®¾ç½®å¥å­çº§åˆ«æ•°æ®é›†
ds = convert_spans_to_dataset(docs, sentence_level=False)
dsd = train_val_split(ds, val_frac=0.2)
# å¯ä»¥è½¬æ¢ä¸ºdataframeåˆ†ææ•°æ®
df = dsd.to_pandas()
max_length = df['text'].str.len().max()
# ä¿å­˜æ•°æ®é›†,æ³¨æ„è¦ä¿å­˜åˆ°datasets/ç›®å½•ä¸‹
dsd.save_to_disk('datasets/your_dataset_name')
```
> è®­ç»ƒæ¨¡å‹
ç¼–å†™è®­ç»ƒè„šæœ¬
- å•å¡
```bash
nlhappy \
datamodule=span_classification \
datamodule.dataset=your_dataset_name \
datamodule.max_length=2000 \
datamodule.batch_size=2 \
datamodule.plm=roberta-wwm-base \
model=global_pointer \
model.lr=3e-5 \
seed=22222
```
- å¤šå¡
```
nlhappy \
trainer=ddp \
datamodule=span_classification \
datamodule.dataset=dataset_name \
datamodule.max_length=350 \
datamodule.batch_size=2 \
datamodule.plm=roberta-wwm-base \
model=global_pointer \
model.lr=3e-5 \
seed=22222
```
- åå°è®­ç»ƒ
```
nohup bash scripts/train.sh >/dev/null 2>&1 &
```
- ç°åœ¨å¯ä»¥å»[wandbå®˜ç½‘](https://wandb.ai/)æŸ¥çœ‹è®­ç»ƒè¯¦æƒ…äº†, å¹¶ä¸”ä¼šè‡ªåŠ¨äº§ç”Ÿlogsç›®å½•é‡Œé¢åŒ…å«äº†è®­ç»ƒçš„ckpt,æ—¥å¿—ç­‰ä¿¡æ¯.
> æ„å»ºè‡ªç„¶è¯­è¨€å¤„ç†æµç¨‹,å¹¶æ·»åŠ ç»„ä»¶
```python
import nlhappy

nlp = nlhappy.nlp()
# é»˜è®¤device cpu, é˜ˆå€¼0.8
config = {'device':'cuda:0', 'threshold':0.9, 'set_ents':True}
tc = nlp.add_pipe('span_classifier', config=config)
# logsæ–‡ä»¶å¤¹é‡Œé¢è®­ç»ƒçš„æ¨¡å‹è·¯å¾„
ckpt = 'logs/experiments/runs/your_best_ckpt_path'
tc.init_model(ckpt)
text = 'æ–‡æœ¬'
doc = nlp(text)
# æŸ¥çœ‹ç»“æœ
# doc.ents ä¸ºéåµŒå¥—å®ä½“,å¦‚æœæœ‰åµŒå¥—ä¼šé€‰æœ€å¤§è·¨åº¦å®ä½“
# doc.spans['all'] å¯ä»¥åŒ…å«åµŒå¥—å®ä½“
print(doc.text, doc.ents, doc.spans['all'])
# ä¿å­˜æ•´ä¸ªæµç¨‹
nlp.to_disk('path/nlp')
# åŠ è½½
nlp = nlhappy.load('path/nlp')
```
> badcaseåˆ†æ
```python
import nlhappy
from nlhappy.utils.analysis_doc import analysis_ent_badcase, Example, analysis_span_badcase
from nlhappy.utils.make_doc import get_docs_from_docbin

targs = get_docs_from_docbin('corpus/dataset_name/train.spacy')
nlp = nlhappy.load('path/nlp')
preds = []
for d in targs:
    doc = nlp(d['text'])
    preds.append(doc)
eg = [Example(x,y) for x,y in zip(preds, targs)]
# éåµŒå¥—å®ä½“
badcases, score = analysis_ent_badcase(eg, return_prf=True)
print(badcases[0].x, badcases[0].x.ents)
print(badcases[0].y, badcases[0].y.ents)
# åµŒå¥—å®ä½“
badcases, score = analysis_span_badcase(eg, return_prf=True)
print(badcases[0].x, badcases[0].x.spans['all'])
print(badcases[0].y, badcases[0].y.spans['all'])
```
> éƒ¨ç½²
- ç›´æ¥ç”¨nlpå¼€å‘æ¥å£éƒ¨ç½²
- è½¬ä¸ºonnx
```python
from nlhappy.models import GlobalPointer
ckpt = 'logs/path/ckpt'
model = GlobalPointer.load_from_ckeckpoint(ckpt)
model.to_onnx('path/tc.onnx')
model.tokenizer.save_pretrained('path/tokenizer')
```
</details>

<details>
<summary><b>å®ä½“æ ‡å‡†åŒ–</b></summary>
TODO
</details>

<details>
<summary><b>å…³ç³»æŠ½å–</b></summary>
TODO
</details>

<details>
<summary><b>äº‹ä»¶æŠ½å–</b></summary>
TODO
</details>

<details>
<summary><b>é€šç”¨ä¿¡æ¯æŠ½å–</b></summary>
TODO
</details>

<details>
<summary><b>æ‘˜è¦</b></summary>
TODO
</details>

<details>
<summary><b>ç¿»è¯‘</b></summary>
TODO
</details>