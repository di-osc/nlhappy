from spacy.pipeline import Sentencizer
from spacy.lang.zh import Chinese
from spacy.language import Language
from typing import List, Optional, Tuple, Callable
from spacy.pipeline import Pipe
import spacy


default_punct_chars = ['!',  '?', 'Ö‰', 'ØŸ', 'Û”', 'Ü€', 'Ü', 'Ü‚', 'ß¹',
            'à¥¤', 'à¥¥', 'áŠ', 'á‹', 'á¢', 'á§', 'á¨', 'á™®', 'áœµ', 'áœ¶', 'á ƒ', 'á ‰', 'á¥„',
            'á¥…', 'áª¨', 'áª©', 'áªª', 'áª«', 'á­š', 'á­›', 'á­', 'á­Ÿ', 'á°»', 'á°¼', 'á±¾', 'á±¿',
            'â€¼', 'â€½', 'â‡', 'âˆ', 'â‰', 'â¸®', 'â¸¼', 'ê“¿', 'ê˜', 'ê˜', 'ê›³', 'ê›·', 'ê¡¶',
            'ê¡·', 'ê£', 'ê£', 'ê¤¯', 'ê§ˆ', 'ê§‰', 'ê©', 'ê©', 'ê©Ÿ', 'ê«°', 'ê«±', 'ê¯«', 'ï¹’',
            'ï¹–', 'ï¹—', 'ï¼', 'ï¼', 'ï¼Ÿ', 'ğ©–', 'ğ©—', 'ğ‘‡', 'ğ‘ˆ', 'ğ‘‚¾', 'ğ‘‚¿', 'ğ‘ƒ€',
            'ğ‘ƒ', 'ğ‘…', 'ğ‘…‚', 'ğ‘…ƒ', 'ğ‘‡…', 'ğ‘‡†', 'ğ‘‡', 'ğ‘‡', 'ğ‘‡Ÿ', 'ğ‘ˆ¸', 'ğ‘ˆ¹', 'ğ‘ˆ»', 'ğ‘ˆ¼',
            'ğ‘Š©', 'ğ‘‘‹', 'ğ‘‘Œ', 'ğ‘—‚', 'ğ‘—ƒ', 'ğ‘—‰', 'ğ‘—Š', 'ğ‘—‹', 'ğ‘—Œ', 'ğ‘—', 'ğ‘—', 'ğ‘—', 'ğ‘—',
            'ğ‘—‘', 'ğ‘—’', 'ğ‘—“', 'ğ‘—”', 'ğ‘—•', 'ğ‘—–', 'ğ‘——', 'ğ‘™', 'ğ‘™‚', 'ğ‘œ¼', 'ğ‘œ½', 'ğ‘œ¾', 'ğ‘©‚',
            'ğ‘©ƒ', 'ğ‘ª›', 'ğ‘ªœ', 'ğ‘±', 'ğ‘±‚', 'ğ–©®', 'ğ–©¯', 'ğ–«µ', 'ğ–¬·', 'ğ–¬¸', 'ğ–­„', 'ğ›²Ÿ', 'ğªˆ',
            'ï½¡', 'ã€‚', 'ï¼Ÿ', 'ï¼', '......', 'â€¦â€¦', ';', 'ï¼›', '.']

@Chinese.factory(name='sentence_segmenter', default_config={'punct_chars': default_punct_chars})
def make_sentence_segmenter(nlp, name, punct_chars):
    return Sentencizer(name, punct_chars=punct_chars)


if __name__ == '__main__':
    nlp = spacy.blank('zh')
    nlp.add_pipe('sentence_segmenter')
    text = "è‚¥åšå‹å¿ƒè‚Œç—…@æœ‰é˜µå‘æ€§æˆ–æ…¢æ€§å¿ƒæˆ¿é¢¤åŠ¨çš„ HCM æ‚£è€…ä½¿ç”¨åæ³•æ—æŠ—å‡çš„å›½é™…æ ‡å‡†åŒ–æ¯”å€¼ (INR) ç›®æ ‡å€¼æ¨èä¸º 2.0-3.0ã€‚"
    doc = nlp(text)
    print(list(doc.sents))

